# -*- coding: utf-8 -*-
"""Regression - real life example.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ibfJAataYFNvjHdPAdtz_zWKQf31BgL2
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
from sklearn.linear_model import LinearRegression

from google.colab import drive
drive.mount('/content/drive')

raw_data = pd.read_csv("/content/drive/MyDrive/1.04.+Real-life+example.csv")
raw_data.head()

raw_data.describe() #we only get descriptives for numerical values

"""## Pre-Processing

Exploring the descriptive statistics of the variable
"""

raw_data.describe(include = 'all')
#now, we have descriptors for categorical variables, too

"""**Determining the variables of interest**

DataFrame.drop(columns, axis) returns a new object with indicated columns cropped.
"""

data = raw_data.drop(['Model'], axis =1 )
data

"""Dealing with missing values"""

data.isnull()
#true - missing; false - available

#sum all missing values and give us the no. of null observations
data.isnull().sum()

original_count = len(data)
rows_with_missing_values = data.isnull().any(axis=1).sum()
print("Number of rows with missing values:", rows_with_missing_values)

print("Original count:", original_count)
print("Cleaned count:", original_count - rows_with_missing_values)

data_no_mv = data.dropna(axis = 0) #removing rows for no missing values(true);
data_no_mv.describe(include = 'all')

"""Exploring the PDFs"""

sns.distplot(data_no_mv['Price'])
#normal distri is expected

"""### Dealing with outliers
DataFrame.quantile(the quantile)
"""

q = data_no_mv['Price'].quantile(0.99)
data_1 = data_no_mv[data_no_mv['Price']<q]
data_1.describe(include='all')

sns.distplot(data_1['Price'])

sns.distplot(data_no_mv['Mileage'])

q = data_1['Mileage'].quantile(0.99)
data_2 = data_no_mv[data_no_mv['Mileage']<q]
data_2.describe(include='all')

sns.distplot(data_2['Mileage'])

sns.distplot(data_no_mv['EngineV'])
#practically, engine values exist between 0.6 and 0.65 , so 99 like values are useless and meaningless

data_3 = data_2[data_2['EngineV']<6.5]
sns.displot(data_3['EngineV'])

sns.displot(data_no_mv['Year'])

q = data_3['Year'].quantile(0.01)
data_4= data_3[data_3['Year']>q]
sns.distplot(data_4['Year'])

#To reset new index for dataset as it is cleaned now(outliers and missing data removed now)- argument drop=True is added to completely forget the old index
data_cleaned = data_4.reset_index(drop= True)
data_cleaned.describe(include = 'all')

"""## Checking the OLS assumptions"""

#checking linearity using scatter plot
f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize =(15,3)) #sharey -> share 'Price' as y
ax1.scatter(data_cleaned['Year'],data_cleaned['Price'])
ax1.set_title('Price and Year')
ax2.scatter(data_cleaned['EngineV'],data_cleaned['Price'])
ax2.set_title('Price and EngineV')
ax3.scatter(data_cleaned['Mileage'],data_cleaned['Price'])
ax3.set_title('Price and Mileage')

plt.show()

# From the subplots and the PDF of price, we can easily determine that 'Price' is exponentially distributed
# A good transformation in that case is a log transformation - #
sns.distplot(data_cleaned['Price'])

"""### Relaxing the assumptions"""

log_price = np.log(data_cleaned['Price']) #np.log(x)
data_cleaned['log_price'] = log_price
data_cleaned

#to check linear plots
f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize =(15,3)) #sharey -> share 'Price' as y
ax1.scatter(data_cleaned['Year'],data_cleaned['log_price'])
ax1.set_title('log_price and Year')
ax2.scatter(data_cleaned['EngineV'],data_cleaned['log_price'])
ax2.set_title('log_price and EngineV')
ax3.scatter(data_cleaned['Mileage'],data_cleaned['log_price'])
ax3.set_title('log_price and Mileage')

plt.show()

data_cleaned = data_cleaned.drop(['Price'],axis=1) #removing price col from dataset now

"""# Multicollinearity"""

data_cleaned.columns.values

# sklearn does not have a built-in way to check for multicollinearity
# one of the main reasons is that this is an issue well covered in statistical frameworks and not in ML ones
# surely it is an issue nonetheless, thus we will try to deal with it

# Here's the relevant module
# full documentation: http://www.statsmodels.org/dev/_modules/statsmodels/stats/outliers_influence.html#variance_inflation_factor
from statsmodels.stats.outliers_influence import variance_inflation_factor

# To make this as easy as possible to use, we declare a variable where we put
# all features where we want to check for multicollinearity
# since our categorical data is not yet preprocessed, we will only take the numerical ones
variables = data_cleaned[['Mileage','Year','EngineV']]

# we create a new data frame which will include all the VIFs
# note that each variable has its own variance inflation factor as this measure is variable specific (not model specific)
vif = pd.DataFrame()

# here we make use of the variance_inflation_factor, which will basically output the respective VIFs
vif["VIF"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]
# Finally, I like to include names so it is easier to explore the result
vif["Features"] = variables.columns
vif

#links to study vif
#https://statisticalhorizons.com/multicollinearity/
#https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html
data_no_multicollinearity = data_cleaned.drop(['Year'],axis = 1)

"""# PRE-PROCESSING
Create dummy variables
"""

data_with_dummies = pd.get_dummies(data_no_multicollinearity, drop_first= True)
# Ensure binary columns are 0 and 1
for col in data_with_dummies.select_dtypes(include=[bool]):
    data_with_dummies[col] = data_with_dummies[col].astype(int)

data_with_dummies.head()

"""Rearrange a bit"""

# To make our data frame more organized, we prefer to place the dependent variable in the beginning of the df
# Since each problem is different, that must be done manually
# We can display all possible features and then choose the desired order
data_with_dummies.columns.values

# To make the code a bit more parametrized, let's declare a new variable that will contain the preferred order
# If you want a different order, just specify it here
# Conventionally, the most intuitive order is: dependent variable, indepedendent numerical variables, dummies
cols = ['log_price', 'Mileage', 'EngineV', 'Brand_BMW',
       'Brand_Mercedes-Benz', 'Brand_Mitsubishi', 'Brand_Renault',
       'Brand_Toyota', 'Brand_Volkswagen', 'Body_hatch', 'Body_other',
       'Body_sedan', 'Body_vagon', 'Body_van', 'Engine Type_Gas',
       'Engine Type_Other', 'Engine Type_Petrol', 'Registration_yes']

# To implement the reordering, we will create a new df, which is equal to the old one but with the new order of features
data_preprocessed = data_with_dummies[cols]
data_preprocessed.head()

"""# EXERCISE

### Part 1
Calculate the variance inflation factors for all variables contained in data_preprocessed.

### Part 2
Our task is to calculate the variance inflation factor (VIF) of all variables including the dummies (but without the dependent variable).

### Part 3
Now calculate the VIFs for a data frame where we include the dummies, without 'log_price', but DO NOT DROP THE FIRST DUMMY.
"""

# Let's simply use the data_preprocessed and the VIF code from above
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Ensure data_preprocessed contains only numeric data
data_preprocessed = data_preprocessed.apply(pd.to_numeric, errors='coerce')

# Handle missing values by filling them in or dropping rows/columns with missing values
# Here, we choose to drop columns with any missing values
data_preprocessed = data_preprocessed.dropna(axis=1, how='any')

# Calculate VIF
variables = data_preprocessed
vif = pd.DataFrame()
vif["VIF"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]
vif["features"] = variables.columns

print(vif)

"""Obviously, 'log_price' has a very high VIF. This implies it is most definitely **linearly correlated** with all the other variables. And this is no surprise! We are using a linear regression to determine 'log_price' given values of the independent variables! This is exactly what we expect - a linear relationship!

However, to actually assess multicollinearity for the predictors, we have to drop 'log_price'. The multicollinearity assumption refers only to the idea that the **independent variables** shoud not be collinear.
"""

# Let's simply drop log_price from data_preprocessed
variables = data_preprocessed.drop(['log_price'],axis=1)
vif = pd.DataFrame()
vif["VIF"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]
vif["features"] = variables.columns
vif

"""As you can see, all VIFs are pretty much acceptable. The ones that are particularly high are 'EngineV' and 'Registration_yes'.

We already discussed 'EngineV' in the lecture.

In the case of registration, the main issue is that **most values are 'yes'** so all types of problems come from there. One way this imbalance manifests is in multicollinearity. Remember that all independent variables are pretty good at determining 'log_price'? Well, if 'registration' is always 'yes', then if we predict 'log_price' we are predicting registration, too (it is going to be 'yes'). That is why, whenever a single category is so predominant, we may just drop the variable.

Note that it will most probably be insignificant anyways.
"""

# To solve this one, we must create a new variable with dummies, without dropping the first one
data_with_dummies_new = pd.get_dummies(data_no_multicollinearity)#, drop_first=True)

# Ensure binary columns are 0 and 1
for col in data_with_dummies_new.select_dtypes(include=[bool]):
    data_with_dummies_new[col] = data_with_dummies_new[col].astype(int)

data_with_dummies_new.head()

# Let's simply drop 'log_price' from this new variable
variables = data_with_dummies_new.drop(['log_price'],axis=1)
vif = pd.DataFrame()
vif["VIF"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]
vif["features"] = variables.columns
vif

"""The result that we get is that very interesting indeed. Most VIFs are equal to **inf**, or plus infinity.

We even got an warning: *RuntimeWarning: divide by zero encountered in double_scalars, vif = 1. / (1. - r_squared_i)*

The main reason is what we've discussed before. When a car is an 'Audi' all other brand dummies are 0. When a car is not 'Audi', at least one of them will be 1. By including all dummies have introduced multicollinearity (**perfect multicollinearity**)!!!

If we run a regression including all these dummies, the coefficients would be inflated and completely off-mark.

Now you see why we need to drop one of the dummy variables for each feature.

# Linear Regression Model
Declare inputs and targets
"""

targets = data_preprocessed['log_price']
inputs = data_preprocessed.drop(['log_price'], axis=1)

"""Scale the data"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(inputs)
input_scaled = scaler.transform(inputs)

"""# Train_test_Split"""

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(input_scaled, targets, test_size =0.2, random_state= 365)

"""# Linear Regression"""

reg = LinearRegression()
reg.fit(x_train, y_train)
y_hat = reg.predict(x_train)

plt.scatter(y_train, y_hat)
plt.xlabel('Targets(y_train)', size = 18)
plt.ylabel('Predictions(y_hat)', size =18)
plt.xlim(6,13)
plt.ylim(6,13)
plt.show()

"""Residuals are differencces between the targets and predictions. They are estimates of errors.

"""

sns.displot(y_train - y_hat)
plt.title("Resdiuals PDF", size = 18)


# In the best case scenario this plot should be normally distributed
# In our case we notice that there are many negative residuals (far away from the mean)
# Given the definition of the residuals (y_train - y_hat), negative values imply
# that y_hat (predictions) are much higher than y_train (the targets)
# This is food for thought to improve our model

# Find the R-squared of the model
reg.score(x_train,y_train)

# Note that this is NOT the adjusted R-squared
# in other words... find the Adjusted R-squared to have the appropriate measure :)

"""Finding the weights and bias"""

reg.intercept_

reg.coef_

reg_summary = pd.DataFrame(inputs.columns.values, columns=['Features'])
reg_summary['Weights'] = reg.coef_
reg_summary

# Check the different categories in the 'Brand' variable
data_cleaned['Brand'].unique()

# In this way we can see which 'Brand' is actually the benchmark
#Audi is missing, so it is mid value ; hence, benchmark.

data_cleaned['Body'].unique()
#crossover is benchmark

"""# Testing"""

y_hat_test = reg.predict(x_test)

plt.scatter(y_test, y_hat_test, alpha =0.2)
plt.xlabel('Targets (y_test)', size = 18)
plt.ylabel('Predictions (y_hat_test)', size = 18)
plt.xlim(6, 13)
plt.ylim(6, 13)
plt.show()

#data frame performance
# To obtain the actual prices, we take the exponential of the log_price
df_pf = pd.DataFrame(np.exp(y_hat_test), columns=['Prediction'])
df_pf.head()

# We can also include the test targets in that data frame (so we can manually compare them)
df_pf['Target'] = np.exp(y_test)
df_pf

# Note that we have a lot of missing values
# There is no reason to have ANY missing values, though
# This suggests that something is wrong with the data frame / indexing

y_test = y_test.reset_index(drop=True) #this will reset the index of y_test
y_test.head()

df_pf['Target'] = np.exp(y_test)
df_pf
#we get the results we initially tried to achieve

# Additionally, we can calculate the difference between the targets and the predictions
# Note that this is actually the residual (we already plotted the residuals)
df_pf['Residual'] = df_pf['Target'] - df_pf['Prediction']

# Since OLS is basically an algorithm which minimizes the total sum of squared errors (residuals),
# this comparison makes a lot of sense

# Finally, it makes sense to see how far off we are from the result percentage-wise
# Here, we take the absolute difference in %, so we can easily order the data frame
df_pf['Difference%'] = np.absolute(df_pf['Residual']/df_pf['Target']*100)
df_pf

df_pf.describe()

# Sometimes it is useful to check these outputs manually
# To see all rows, we use the relevant pandas syntax
pd.options.display.max_rows = 999
# Moreover, to make the dataset clear, we can display the result with only 2 digits after the dot
pd.set_option('display.float_format', lambda x: '%.2f' % x)
# Finally, we sort by difference in % and manually check the model
df_pf.sort_values(by=['Difference%'])

from sklearn.metrics import mean_absolute_error, mean_squared_error

# Calculate MAE
mae = mean_absolute_error(y_test, y_hat_test)
print(f'Mean Absolute Error (MAE): {mae}')

# Calculate MSE
mse = mean_squared_error(y_test, y_hat_test)
print(f'Mean Squared Error (MSE): {mse}')

def mean_absolute_percentage_error(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

mape = mean_absolute_percentage_error(y_test, y_hat_test)
print(f'Mean Absolute Percentage Error (MAPE): {mape}%')
#A lower MAPE indicates a better model, with a MAPE of 0% meaning perfect predictions.